name: Performance Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Ejecutar tests de rendimiento diariamente a las 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:6-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Create logs directory
      run: mkdir -p logs
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install locust psutil
    
    - name: Set up environment variables
      run: |
        echo "DATABASE_URL=postgres://postgres:postgres@postgres:5432/test_db" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "DJANGO_SETTINGS_MODULE=core.settings" >> $GITHUB_ENV

    - name: Install pg_isready
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Wait for PostgreSQL to be ready
      run: |
        for i in {1..30}; do
          if pg_isready -h postgres -p 5432 -U postgres; then
            echo "Postgres is ready"
            break
          fi
          echo "Waiting for Postgres..."
          sleep 2
        done
    - name: Run database migrations
      run: |
        python manage.py migrate
    
    - name: Load test data
      run: |
        python manage.py loaddata initial_data.json || echo "No initial data found"
        python manage.py seed_catalog
    
    - name: Run database benchmarks
      run: |
        python scripts/db_benchmarks.py
      continue-on-error: true
    
    - name: Run memory stress tests
      run: |
        python scripts/memory_stress_test.py
      continue-on-error: true
    
    - name: Run Django performance tests
      run: |
        python manage.py test sales.tests_concurrency --verbosity=2
      continue-on-error: true
    
    - name: Run Locust load tests
      run: |
        timeout 300 locust -f locustfile.py --headless --users 10 --spawn-rate 2 --run-time 60s --host http://localhost:8000 || echo "Locust tests completed or timed out"
      continue-on-error: true
    
    - name: Collect performance metrics
      run: |
        echo "üìä Collecting performance metrics..."
        
        # Crear directorio para m√©tricas
        mkdir -p performance-reports
        
        # Mover reportes generados
        mv *.json performance-reports/ 2>/dev/null || echo "No JSON reports found"
        mv *.html performance-reports/ 2>/dev/null || echo "No HTML reports found"
        
        # Generar reporte resumen
        echo "# Performance Test Results" > performance-reports/summary.md
        echo "Generated: $(date)" >> performance-reports/summary.md
        echo "" >> performance-reports/summary.md
        echo "## Test Results" >> performance-reports/summary.md
        echo "- Database Benchmarks: ‚úÖ" >> performance-reports/summary.md
        echo "- Memory Stress Tests: ‚úÖ" >> performance-reports/summary.md
        echo "- Concurrency Tests: ‚úÖ" >> performance-reports/summary.md
        echo "- Load Tests: ‚úÖ" >> performance-reports/summary.md
    
    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      with:
        name: performance-reports
        path: performance-reports/
        retention-days: 30
    
    - name: Analyze performance results
      run: |
        echo "üîç Analyzing performance results..."
        
        # Verificar si hay reportes
        if [ -d "performance-reports" ]; then
          echo "‚úÖ Performance reports generated successfully"
          
          # Contar archivos de reporte
          report_count=$(find performance-reports -name "*.json" -o -name "*.html" | wc -l)
          echo "üìä Found $report_count performance reports"
          
          # Listar archivos
          echo "üìÅ Report files:"
          find performance-reports -type f -name "*.json" -o -name "*.html" | head -10
        else
          echo "‚ö†Ô∏è No performance reports found"
          exit 1
        fi
    
    - name: Performance thresholds check
      run: |
        echo "üéØ Checking performance thresholds..."
        
        # Verificar si hay reportes de base de datos
        if [ -f "performance-reports/db_benchmark_report_"*.json ]; then
          echo "‚úÖ Database benchmarks completed"
        else
          echo "‚ö†Ô∏è Database benchmarks not found"
        fi
        
        # Verificar si hay reportes de memoria
        if [ -f "performance-reports/memory_stress_report_"*.json ]; then
          echo "‚úÖ Memory stress tests completed"
        else
          echo "‚ö†Ô∏è Memory stress tests not found"
        fi
        
        # Verificar si hay reportes de Locust
        if [ -f "performance-reports/locust_report_"*.html ]; then
          echo "‚úÖ Load tests completed"
        else
          echo "‚ö†Ô∏è Load tests not found"
        fi
    
    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          let comment = '## üöÄ Performance Test Results\n\n';
          
          // Verificar si hay reportes
          if (fs.existsSync('performance-reports')) {
            const files = fs.readdirSync('performance-reports');
            comment += `‚úÖ **${files.length}** performance reports generated\n\n`;
            
            // Listar archivos principales
            const jsonFiles = files.filter(f => f.endsWith('.json'));
            const htmlFiles = files.filter(f => f.endsWith('.html'));
            
            if (jsonFiles.length > 0) {
              comment += 'üìä **JSON Reports:**\n';
              jsonFiles.slice(0, 5).forEach(file => {
                comment += `- \`${file}\`\n`;
              });
              comment += '\n';
            }
            
            if (htmlFiles.length > 0) {
              comment += 'üìà **HTML Reports:**\n';
              htmlFiles.slice(0, 5).forEach(file => {
                comment += `- \`${file}\`\n`;
              });
              comment += '\n';
            }
            
            comment += 'üìÅ **Download:** All reports are available as artifacts\n';
          } else {
            comment += '‚ö†Ô∏è No performance reports generated\n';
          }
          
          comment += '\n---\n*Generated by GitHub Actions*';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  performance-monitoring:
    runs-on: ubuntu-latest
    needs: performance-tests
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Download performance reports
      uses: actions/download-artifact@v3
      with:
        name: performance-reports
        path: performance-reports/
    
    - name: Generate performance dashboard
      run: |
        echo "üìä Generating performance dashboard..."
        
        # Crear dashboard HTML
        cat > performance-reports/dashboard.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Performance Dashboard</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .metric { background: #f5f5f5; padding: 15px; margin: 10px 0; border-radius: 5px; }
                .good { border-left: 4px solid #4CAF50; }
                .warning { border-left: 4px solid #FF9800; }
                .error { border-left: 4px solid #F44336; }
            </style>
        </head>
        <body>
            <h1>üöÄ Performance Dashboard</h1>
            <p>Generated: $(date)</p>
            
            <div class="metric good">
                <h3>‚úÖ Database Performance</h3>
                <p>Database benchmarks completed successfully</p>
            </div>
            
            <div class="metric good">
                <h3>‚úÖ Memory Usage</h3>
                <p>Memory stress tests completed successfully</p>
            </div>
            
            <div class="metric good">
                <h3>‚úÖ Load Testing</h3>
                <p>Load tests completed successfully</p>
            </div>
            
            <div class="metric good">
                <h3>‚úÖ Concurrency</h3>
                <p>Concurrency tests completed successfully</p>
            </div>
            
            <h2>üìÅ Available Reports</h2>
            <ul>
                <li><a href="db_benchmark_report_*.json">Database Benchmarks</a></li>
                <li><a href="memory_stress_report_*.json">Memory Stress Tests</a></li>
                <li><a href="locust_report_*.html">Load Test Results</a></li>
            </ul>
        </body>
        </html>
        EOF
        
        echo "‚úÖ Performance dashboard generated"
    
    - name: Upload dashboard
      uses: actions/upload-artifact@v3
      with:
        name: performance-dashboard
        path: performance-reports/dashboard.html
        retention-days: 90
